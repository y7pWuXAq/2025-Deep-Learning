{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a9287d",
   "metadata": {},
   "source": [
    "1. 패션데이터셋 읽어들이기 : 변수는 기존과 동일\n",
    "2. 데이터 스케일링 : 변수는 기존과 동일\n",
    "3. 2차원 데이터로 변경\n",
    "4. 훈련 : 검증 = 8:2로 분리 : 변수는 기존과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 텐서플로 프레임워크 사용\n",
    "# - 사용 라이브러리 : keras\n",
    "\n",
    "### 텐서플로우 프레임워크(패키지라고도 칭함)\n",
    "import tensorflow as tf\n",
    "\n",
    "### 케라스 라이브러리 불러들이기\n",
    "from tensorflow import keras\n",
    "\n",
    "### 시각화 라이브러리 불러들이기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### 넘파이\n",
    "import numpy as np\n",
    "\n",
    "### 훈련 및 테스트 데이터로 분류하는 라이브러리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### 딥러닝 랜덤 규칙 정의하기\n",
    "# - 딥러닝에서의 랜덤 규칙은 항상 일정하게 유지되지는 않음\n",
    "# - 딥러닝 모델 내부에서 훈련을 위한 데이터를 임의로 추출하여 사용(사람이 관여 안함)\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "# tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "### keras에서 제공하는 데이터 라이브러리 : Fashion MNIST\n",
    "# - 이미지 데이터로 되어있음\n",
    "\n",
    "### 데이터셋 읽어들이기\n",
    "# - 반환되는 데이터 : train(독립, 종속), test(독립, 종속) 총 4개 데이터 변수로 반환됨\n",
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(train_input.shape, train_target.shape)\n",
    "print(test_input.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이미지 픽셀 데이터를 0~1사이의 값으로 변환하기\n",
    "# - 이미지 픽셀 데이터의 범위는 0~255의 범위값으로 구성되어 있음\n",
    "# - 따라서, 각 픽셀의 값을 255로 나눠주면, 0~1사이의 값으로 일반화가 가능함\n",
    "# - 훈련 및 테스트 독립변수의 데이터를 모두 255로 나누어서 전처리하기\n",
    "#   (255.0의 소숫점으로 나눠줌)\n",
    "train_scaled_255 = train_input / 255.0\n",
    "test_scaled_255  = test_input / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c063318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 2차원 데이터로 변환\n",
    "train_scaled_2d = train_scaled_255.reshape(-1, 28 * 28)\n",
    "test_scaled_2d = test_scaled_255.reshape(-1, 28 * 28)\n",
    "\n",
    "train_scaled_2d.shape, test_scaled_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8defa178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (48000,)\n",
      "(12000, 784) (12000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "### 훈련(train_scaled, train_target)\n",
    "### 검증(val_scaled, val_target)\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(\n",
    "    train_scaled_2d, train_target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(train_scaled.shape,   train_target.shape)\n",
    "print(val_scaled.shape,     val_target.shape)\n",
    "print(test_scaled_2d.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38412866",
   "metadata": {},
   "source": [
    "### 신경망 모델에 계층(Layer) 추가하는 방법(3가지)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544f89b",
   "metadata": {},
   "source": [
    "#### 1. 계층(Layer)을 먼저 만들고, 신경망 모델 생성 시 추가하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87fb9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x220971099a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 입력계층 생성하기\n",
    "# - 변수명 : dense1\n",
    "# - 활성화 함수 : sigmoid 사용\n",
    "# - 출력 데이터 갯수 : 100개\n",
    "# - 입력 데이터 갯수 : 784개\n",
    "dense1 = keras.layers.Dense(\n",
    "    # 출력갯수\n",
    "    units = 100,\n",
    "    # 활성화 함수\n",
    "    activation = \"sigmoid\",\n",
    "    # 입력 데이터 갯수\n",
    "    input_shape = (784, )\n",
    ")\n",
    "dense1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa284e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x22113964910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 출력계층 생성하기\n",
    "# - 변수명 : dense2\n",
    "dense2 = keras.layers.Dense(\n",
    "    # 최종 출력갯수(종속변수 범주 갯수)\n",
    "    units = 10,\n",
    "    # 다중분류\n",
    "    activation = \"softmax\"\n",
    ")\n",
    "dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bc2eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### 신경망 모델 생성과 동시에 미리 생성한 계층 추가하기\n",
    "model = keras.Sequential([dense1, dense2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd23f37",
   "metadata": {},
   "source": [
    "#### 2. 신경망 모델 생성 시에 계층(Layer)을 함께 추가하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208af992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model2\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_Layer (Dense)         (None, 100)               78500     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### 신경망 모델 생성하기\n",
    "model2 = keras.Sequential([\n",
    "    # 입력계층 생성 및 추가\n",
    "    keras.layers.Dense(units=100, activation=\"sigmoid\",\n",
    "                        input_shape=(784,), name=\"Input_Layer\"),\n",
    "\n",
    "    # 출력계층 생성 및 추가\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\", name=\"Output-Layer\")\n",
    "\n",
    "   # 모델 이름 정의 \n",
    "], name=\"Model2\")\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabb99f",
   "metadata": {},
   "source": [
    "#### 3. 신경망 모델을 먼저 생성 후 add()함수를 이용해서 계층(Layer) 추가하는 방식(가장 많이 사용됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d565cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17475c0b730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델 생성하기\n",
    "model3 = keras.Sequential(name=\"Model3\")\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ab1ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (Dense)         (None, 100)               78500     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,500\n",
      "Trainable params: 78,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### 입력계층 생성 및 추가하기\n",
    "model3.add(\n",
    "    keras.layers.Dense(units=100, activation=\"sigmoid\", \n",
    "                        input_shape=(784,), name=\"Input-Layer\")\n",
    ")\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d996ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (Dense)         (None, 100)               78500     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### 출력계층 생성 및 모델에 추가하기\n",
    "model3.add(\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\", name=\"Output-Layer\")\n",
    ")\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa0bba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel3\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/CPU:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     model3\u001b[38;5;241m.\u001b[39mfit(train_scaled, train_target, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model3' is not defined"
     ]
    }
   ],
   "source": [
    "## 모델 환경 설정하기\n",
    "## 훈련 데이터로 10회만 훈련\n",
    "model3.compile(\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = \"accuracy\"\n",
    ")\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    model3.fit(train_scaled, train_target, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420623f",
   "metadata": {},
   "source": [
    "### 훈련모델 성능 검증(평가) 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a859168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.2253 - accuracy: 0.9179\n",
      "훈련 손실율 : 0.22533667087554932, 훈련 정확도 : 0.9178541898727417\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3408 - accuracy: 0.8844\n",
      "검증 손실율 : 0.3407609760761261, 검증 정확도 : 0.8844166398048401\n"
     ]
    }
   ],
   "source": [
    "### 훈련데이터로 평가하기\n",
    "train_score = model3.evaluate(train_scaled, train_target)\n",
    "print(f\"훈련 손실율 : {train_score[0]}, 훈련 정확도 : {train_score[1]}\")\n",
    "\n",
    "### 검증데이터로 평가하기\n",
    "val_score = model3.evaluate(val_scaled, val_target)\n",
    "print(f\"검증 손실율 : {val_score[0]}, 검증 정확도 : {val_score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df766cdb",
   "metadata": {},
   "source": [
    "### 성능향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079ba14",
   "metadata": {},
   "source": [
    "- 모델 성능 향상 방법\n",
    "    - 데이터 증가시키기\n",
    "    - 하이퍼파라메터 튜닝\n",
    "        - 반복횟수 증/감(epochs)\n",
    "        - 출력계층을 제외한 계층에서의 활성화(activation)함수 변경\n",
    "        - 배치 사이즈\n",
    "        - 출력계층을 제회한 계층에서의 출력 데이터의 갯수(units)\n",
    "        - 은닉계층(Hidden Layer)을 추가 또는 제거(일반적으로 추가 후 영향이 없으면 제거)\n",
    "        - 이외 성능에 영향을 미치는 하이퍼파라메터 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03051de2",
   "metadata": {},
   "source": [
    "#### 성능향상 - 은닉계층(Hidden Layer) 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68da240",
   "metadata": {},
   "source": [
    "1. 모델생성 : 변수명은 model\n",
    "2. 출력계층 생성 : 기존과 동일\n",
    "3. 은닉계층 생성 : 계층은 자유롭게 생성\n",
    "4. 출력계층 생성\n",
    "5. 훈련 횟수 10회로 훈련까지 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1004fe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.5599 - accuracy: 0.8023\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.4035 - accuracy: 0.8523\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3669 - accuracy: 0.8665\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3435 - accuracy: 0.8753\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3247 - accuracy: 0.8800\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.3130 - accuracy: 0.8859\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.3001 - accuracy: 0.8894\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 10s 7ms/step - loss: 0.2888 - accuracy: 0.8927\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.2792 - accuracy: 0.8987\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 11s 7ms/step - loss: 0.2699 - accuracy: 0.9021\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2578 - accuracy: 0.9049\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3360 - accuracy: 0.8862\n",
      "훈련 손실율 : 0.25779613852500916, 훈련 정확도 : 0.9048958420753479\n",
      "검증 손실율 : 0.3360101282596588, 검증 정확도 : 0.8861666917800903\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(name=\"Model\")\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(units=100, activation=\"sigmoid\", \n",
    "                        input_shape=(784,), name=\"Input-Layer\")\n",
    ")\n",
    "model.add(\n",
    "    keras.layers.Dense(units=50, activation=\"relu\", name=\"Hidden-Layer\")\n",
    ")\n",
    "model.add(\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\", name=\"Output-Layer\")\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = \"accuracy\"\n",
    ")\n",
    "\n",
    "with tf.device(\"/CPU:0\") :\n",
    "    model.fit(train_scaled, train_target, epochs=10)\n",
    "\n",
    "train_score = model.evaluate(train_scaled, train_target)\n",
    "val_score = model.evaluate(val_scaled, val_target)\n",
    "print(f\"훈련 손실율 : {train_score[0]}, 훈련 정확도 : {train_score[1]}\")\n",
    "print(f\"검증 손실율 : {val_score[0]}, 검증 정확도 : {val_score[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ac163",
   "metadata": {},
   "source": [
    "#### 성능향상 : 모델 설정 시 옵티아미저(Optimizer) 설정(최적화 기법)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75132d",
   "metadata": {},
   "source": [
    "- 옵티마이저(Optimizer, 최적화) 기법\n",
    "    - 손실을 줄여나가기 위한 최적화(Optimizer) 방법을 의미함\n",
    "    - 손실을 줄여나가는 최적화 방법으로 \"경사하강법\" 이론이 적용\n",
    "    - 특성들의 시작 위치에서 목적지(종속변수 위치)까지 도달하기 위한 기울어진 방향을 찾기 위한 방법을 의미\n",
    "    - \"경사하강법\" 이론을 적용한 여러가지 방법들중 하나를 선택하여 정의\n",
    "    - 옵티마이저(최적화) 방법\n",
    "        - SGD(확률적경사하강법) < Adagrad < RMSProp < Adam, 이외 등등\n",
    "    - 옵티마이저 설정 위치 : model.compile(optimizer=\"옵티마이저(최적화) 방법중 1개\")\n",
    " \n",
    "- 옵티마이저(최적화) 방법 정의\n",
    "    * SGD(확률적 경사하강법)\n",
    "        - 특성이 현재 위치에서 목적지까지 도달하는 과정 중에 보폭을 크게하여 많은 길을 거치면서(극단적으로 방향을 바꿈) 빠르게 탐색\n",
    "    - 지그 재그 모양으로 탐색하면서 나아가는 방법\n",
    "    - 아래 옵티마이저 방법들은 SGD를 근간으로 향상된 방법들임\n",
    "    - 단점 : 보폭을 크게하면서 방향을 근단적으로 바꿈 - > 주변을 정밀하게 확인하기 어려움\n",
    "        - 보폭을 크게하기 때문에, 목적지를 건너 띄는 경우도 발생함(종속변수를 잘 못 찾는 경우 발생)\n",
    "   \n",
    "    * Adagrad\n",
    "        - SGD의 큰 보폭에 대한 단점을 보완한 방법\n",
    "        - 학습률(보폭)을 적절하게 설정하기 위해 학습률 감소(보폭을 짧게)라는 기술 사용\n",
    "        - 학습 진행 중에 학습률을 줄여가는 방법 사용\n",
    "        - 처음에는 학습률(보폭)을 크게 학습하다가 점점 작게(보폭을 짧게) 학습한다는 개념을 적용\n",
    "        - 이미 학습된 곳은 보폭을 크게하고, 학습이 완료되었던 곳은 보폭을 짧게 세밀하게 탐색\n",
    "        - 손실이 더 이상 줄어들지 않으면(손실이 0이면) 종료\n",
    "        - 단점, 목적지에 도달하지 않더라도 손실이 0이면 종료하는 단점이 있음\n",
    " \n",
    "    * RMSProp\n",
    "        - Adagrad는 학습량을 점점 작게 학습하기 때문에 학습률(보폭)이 0이 되어 갱신되지 않는(학습되지 않는) 시점이 발생할 수 있는 단점이 있음\n",
    "        - Adagrad의 단점을 보완하여 과거(이전)의 기울기 값을 반영하는 방식을 적용함\n",
    "        - 먼 과거의 기울기(경사) 값은 조금만 반영하고 최근 기울기(경사)를 많이 반영하는 방식으로 처리됨\n",
    "        - 과거 데이터를 저장해 놓아야 하기 때문에, 다소 훈련 시간이 걸림\n",
    "        - 옵티마이저의 기본값(default)으로 사용됨, 생략가능\n",
    " \n",
    "    * Adam\n",
    "        - 공이 굴러가듯이 모멘텀(Momentum, 관성=방향담당)과 RMSProp을 융합한 방법\n",
    "        - 방향(momentum)과 학습률(보폭)을 적절하게하여 탐색함\n",
    "        - 자주 사용되는 기법으로, 좋은 결과를  얻을 수 있는 방법으로 유명함\n",
    " \n",
    "    * Momentum(모멘텀)\n",
    "        - 관성과 가속도를 적용하여 이동하던 방향으로 좀 더 유연하게 작동함\n",
    "        - 메모리 사용이 많은 단점이 있음 (과거 데이터를 저장해 놓고, 다음 과정에서 방향성(관성)을 이어 받아서 사용하게 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57775b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2334 - accuracy: 0.9149\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2268 - accuracy: 0.9171\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2249 - accuracy: 0.9173\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2233 - accuracy: 0.9183\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2220 - accuracy: 0.9189\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2211 - accuracy: 0.9188\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2200 - accuracy: 0.9191\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2191 - accuracy: 0.9192\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2182 - accuracy: 0.9197\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2174 - accuracy: 0.9204\n"
     ]
    }
   ],
   "source": [
    "### 옵티마이저(optimazer) 적용하기\n",
    "# - 모델 설정시 적용\n",
    "# - 디폴트 옵티마이저 : RMSProp\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = \"accuracy\"\n",
    ")\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    model.fit(train_scaled, train_target, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb5cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.5647 - accuracy: 0.8025\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3988 - accuracy: 0.8560\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3641 - accuracy: 0.8674\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3381 - accuracy: 0.8768\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3168 - accuracy: 0.8830\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3009 - accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2876 - accuracy: 0.8926\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2753 - accuracy: 0.8972\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2638 - accuracy: 0.9019\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2531 - accuracy: 0.9063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1772848fc70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential(name=\"Model\")\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense(units=100, activation=\"sigmoid\", \n",
    "                        input_shape=(784,), name=\"Input-Layer\")\n",
    ")\n",
    "model.add(\n",
    "    keras.layers.Dense(units=50, activation=\"relu\", name=\"Hidden-Layer\")\n",
    ")\n",
    "model.add(\n",
    "    keras.layers.Dense(units=10, activation=\"softmax\", name=\"Output-Layer\")\n",
    ")\n",
    "\n",
    "\n",
    "### 옵티마이저(optimazer) 적용하기\n",
    "# - 모델 설정시 적용\n",
    "# - 디폴트 옵티마이저 : RMSProp\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = \"accuracy\"\n",
    ")\n",
    "\n",
    "# with tf.device(\"/CPU:0\"):\n",
    "model.fit(train_scaled, train_target, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85cb677",
   "metadata": {},
   "source": [
    "##### 성능향상 : 옵티마이저에 학습률(보폭의 크기) 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c31b5",
   "metadata": {},
   "source": [
    "- 학습률(Learning Rate)\n",
    "    - 경사를 내려 올 때의 \"보폭\"이라고 이해\n",
    "    - 옵티마이저 4개는 객체(클래스)로 되어 있음\n",
    "    - 클래스 생성 시에 학습률(learning_rate)을 정의 할 수 있음\n",
    "    - 학습률이 작을 수록 -> 보폭이 작음\n",
    "    - 학습률이 높을 수록 -> 보폭이 큼\n",
    "    - 가장 손실이 작은 위치를 찾아서 움직이도록 작동\n",
    "    - 이때 가장 손실이 작은 위치는 모델이 스스로 찾아주기에 사람이 관여하지 않음\n",
    "    - 학습률(learning_rate) 값의 범위 : 0.1~0.0001 (기본값은 0.01, 생략가능)\n",
    "    - 학습률의 값에 따라서 -> 훈련 성능에 영향을 미침 => 하이퍼파라메터 대상이 됨\n",
    "\n",
    "    - 과적합을 해소하기 위한 튜닝 방법으로 사용되기도 함\n",
    "        * 과대적합이 발생한 경우 : 학습률을 크게(보폭을 크게)\n",
    "        * 과소적합이 발생한 경우 : 학습률을 작게(보폭을 작게)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pk_dl_gpu_kernel",
   "language": "python",
   "name": "pk_dl_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
